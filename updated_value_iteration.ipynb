{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0e73229-094b-4e29-b249-8e0cdd6d0c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'licensekey': '125b........eca7', 'software_license': {}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from quantrocket.license import get_license_profile\n",
    "get_license_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1686b2dc-8805-4426-97b9-64c380cf88ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from quantrocket.history import create_usstock_db, collect_history\n",
    "from quantrocket.master import get_securities\n",
    "from quantrocket.history import download_history_file\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "841c450c-8c3a-4157-92a6-45d0195860ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'licensekey': 'bceb........d2f3', 'software_license': {}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from quantrocket.license import set_license\n",
    "set_license(\"bceb5580-e38b-11ee-bc17-c19b940fd2f3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a7ee9fb-4e5a-4ca1-8b3b-19796a39e4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from quantrocket.history import drop_db\n",
    "#drop_db(\"usstock-free-1d\", confirm_by_typing_db_code_again=\"usstock-free-1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a5a21a2-bdfc-4133-a427-2f5633d58c88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'successfully created quantrocket.v2.history.usstock-free-1d.sqlite'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from quantrocket.history import create_usstock_db\n",
    "create_usstock_db(\"usstock-free-1d\", bar_size=\"1 day\", free=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c19a1403-4672-473a-9a88-a2a12469de09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'the historical data will be collected asynchronously'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from quantrocket.history import collect_history\n",
    "collect_history(\"usstock-free-1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b5ccc5d-3658-485f-9730-f29d6940cdf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Country</th>\n",
       "      <th>Currency</th>\n",
       "      <th>SecType</th>\n",
       "      <th>Etf</th>\n",
       "      <th>Timezone</th>\n",
       "      <th>Name</th>\n",
       "      <th>PriceMagnifier</th>\n",
       "      <th>Multiplier</th>\n",
       "      <th>Delisted</th>\n",
       "      <th>DateDelisted</th>\n",
       "      <th>LastTradeDate</th>\n",
       "      <th>RolloverDate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FIBBG000B9XRY4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>XNAS</td>\n",
       "      <td>US</td>\n",
       "      <td>USD</td>\n",
       "      <td>STK</td>\n",
       "      <td>False</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>APPLE INC</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG000BFWKC0</th>\n",
       "      <td>MON</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>US</td>\n",
       "      <td>USD</td>\n",
       "      <td>STK</td>\n",
       "      <td>False</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>MONSANTO CO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-06-06</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG000BKZB36</th>\n",
       "      <td>HD</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>US</td>\n",
       "      <td>USD</td>\n",
       "      <td>STK</td>\n",
       "      <td>False</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>HOME DEPOT INC</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG000BMHYD1</th>\n",
       "      <td>JNJ</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>US</td>\n",
       "      <td>USD</td>\n",
       "      <td>STK</td>\n",
       "      <td>False</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>JOHNSON &amp; JOHNSON</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG000BPH459</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>XNAS</td>\n",
       "      <td>US</td>\n",
       "      <td>USD</td>\n",
       "      <td>STK</td>\n",
       "      <td>False</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>MICROSOFT CORP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG000CK38G3</th>\n",
       "      <td>KKD</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>US</td>\n",
       "      <td>USD</td>\n",
       "      <td>STK</td>\n",
       "      <td>False</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>KRISPY KREME DOUGHNUTS INC</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-07-27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG000GZQ728</th>\n",
       "      <td>XOM</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>US</td>\n",
       "      <td>USD</td>\n",
       "      <td>STK</td>\n",
       "      <td>False</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>EXXON MOBIL CORP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG00B3T3HD3</th>\n",
       "      <td>AA</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>US</td>\n",
       "      <td>USD</td>\n",
       "      <td>STK</td>\n",
       "      <td>False</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>ALCOA CORP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Symbol Exchange Country Currency SecType    Etf  \\\n",
       "Sid                                                              \n",
       "FIBBG000B9XRY4   AAPL     XNAS      US      USD     STK  False   \n",
       "FIBBG000BFWKC0    MON     XNYS      US      USD     STK  False   \n",
       "FIBBG000BKZB36     HD     XNYS      US      USD     STK  False   \n",
       "FIBBG000BMHYD1    JNJ     XNYS      US      USD     STK  False   \n",
       "FIBBG000BPH459   MSFT     XNAS      US      USD     STK  False   \n",
       "FIBBG000CK38G3    KKD     XNYS      US      USD     STK  False   \n",
       "FIBBG000GZQ728    XOM     XNYS      US      USD     STK  False   \n",
       "FIBBG00B3T3HD3     AA     XNYS      US      USD     STK  False   \n",
       "\n",
       "                        Timezone                        Name  PriceMagnifier  \\\n",
       "Sid                                                                            \n",
       "FIBBG000B9XRY4  America/New_York                   APPLE INC               1   \n",
       "FIBBG000BFWKC0  America/New_York                 MONSANTO CO               1   \n",
       "FIBBG000BKZB36  America/New_York              HOME DEPOT INC               1   \n",
       "FIBBG000BMHYD1  America/New_York           JOHNSON & JOHNSON               1   \n",
       "FIBBG000BPH459  America/New_York              MICROSOFT CORP               1   \n",
       "FIBBG000CK38G3  America/New_York  KRISPY KREME DOUGHNUTS INC               1   \n",
       "FIBBG000GZQ728  America/New_York            EXXON MOBIL CORP               1   \n",
       "FIBBG00B3T3HD3  America/New_York                  ALCOA CORP               1   \n",
       "\n",
       "                Multiplier  Delisted DateDelisted LastTradeDate RolloverDate  \n",
       "Sid                                                                           \n",
       "FIBBG000B9XRY4           1     False          NaT           NaT          NaT  \n",
       "FIBBG000BFWKC0           1      True   2018-06-06           NaT          NaT  \n",
       "FIBBG000BKZB36           1     False          NaT           NaT          NaT  \n",
       "FIBBG000BMHYD1           1     False          NaT           NaT          NaT  \n",
       "FIBBG000BPH459           1     False          NaT           NaT          NaT  \n",
       "FIBBG000CK38G3           1      True   2016-07-27           NaT          NaT  \n",
       "FIBBG000GZQ728           1     False          NaT           NaT          NaT  \n",
       "FIBBG00B3T3HD3           1     False          NaT           NaT          NaT  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from quantrocket.master import get_securities\n",
    "securities = get_securities(vendors=\"usstock\", sec_types=\"STK\")\n",
    "securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b13a03-7a7f-4198-891d-e85c31278e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantrocket.master import create_universe\n",
    "create_universe(\"usstockfrees\", sids=securities.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ea1f08b-fcd6-4443-83bd-bc4599e2b4e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_securities = securities[securities.Delisted==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e55299bd-6548-49e5-9de3-8291f32ab492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Name</th>\n",
       "      <th>Delisted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FIBBG000B9XRY4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>XNAS</td>\n",
       "      <td>APPLE INC</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG000BKZB36</th>\n",
       "      <td>HD</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>HOME DEPOT INC</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG000BMHYD1</th>\n",
       "      <td>JNJ</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>JOHNSON &amp; JOHNSON</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG000BPH459</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>XNAS</td>\n",
       "      <td>MICROSOFT CORP</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG000GZQ728</th>\n",
       "      <td>XOM</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>EXXON MOBIL CORP</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIBBG00B3T3HD3</th>\n",
       "      <td>AA</td>\n",
       "      <td>XNYS</td>\n",
       "      <td>ALCOA CORP</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Symbol Exchange               Name  Delisted\n",
       "Sid                                                        \n",
       "FIBBG000B9XRY4   AAPL     XNAS          APPLE INC     False\n",
       "FIBBG000BKZB36     HD     XNYS     HOME DEPOT INC     False\n",
       "FIBBG000BMHYD1    JNJ     XNYS  JOHNSON & JOHNSON     False\n",
       "FIBBG000BPH459   MSFT     XNAS     MICROSOFT CORP     False\n",
       "FIBBG000GZQ728    XOM     XNYS   EXXON MOBIL CORP     False\n",
       "FIBBG00B3T3HD3     AA     XNYS         ALCOA CORP     False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_securities = filtered_securities[[\"Symbol\", \"Exchange\", \"Name\", \"Delisted\"]]\n",
    "filtered_securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4b572e9-5291-451b-b98d-98d2c4fd6734",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 'usstocks', 'provided': 6, 'inserted': 6, 'total_after_insert': 6}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_universe(\"usstocks\", sids=filtered_securities.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ed9d013-2c9d-4141-9d23-2902cb08f884",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'successfully created quantrocket.v2.history.aapl.sqlite'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_usstock_db(\"aapl\", bar_size=\"1 day\", free=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c3b3c2-8361-41c9-9f0d-0eceba3d5267",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'the historical data will be collected asynchronously'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_history(\"aapl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8393ec8a-761d-49af-9565-551cdf60c86f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "securities = get_securities(vendors=\"usstock\", sec_types=\"STK\")\n",
    "aapl_sid = securities.loc[securities[\"Symbol\"] == \"AAPL\"].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f877a679-2030-4219-93c0-dec4774f92fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from quantrocket import get_prices\n",
    "price_data = get_prices(\"aapl\", sids=[\"FIBBG000B9XRY4\"], start_date=\"2022-01-01\", end_date=\"2022-12-31\", fields=[\"Close\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d86c3205-fbe1-47f3-a222-464682990b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_file_path = \"aapldata.csv\"\n",
    "price_data.to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40e00ba9-7220-48c1-91b9-fe571e6c12a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251, 1)\n"
     ]
    }
   ],
   "source": [
    "print(price_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b74aad69-d9cb-43b2-b3a2-008d1a08d51e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sid               FIBBG000B9XRY4\n",
      "Field Date                      \n",
      "Close 2022-01-03        179.4980\n",
      "      2022-01-04        177.2199\n",
      "      2022-01-05        172.5058\n",
      "      2022-01-06        169.6261\n",
      "      2022-01-07        169.7938\n",
      "...                          ...\n",
      "      2022-12-23        130.9600\n",
      "      2022-12-27        129.1424\n",
      "      2022-12-28        125.1797\n",
      "      2022-12-29        128.7253\n",
      "      2022-12-30        129.0431\n",
      "\n",
      "[251 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(price_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18c5c586-b91f-4687-8950-797dea822b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      179.4980\n",
       "1      177.2199\n",
       "2      172.5058\n",
       "3      169.6261\n",
       "4      169.7938\n",
       "         ...   \n",
       "246    130.9600\n",
       "247    129.1424\n",
       "248    125.1797\n",
       "249    128.7253\n",
       "250    129.0431\n",
       "Name: Close, Length: 251, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "prices = price_data['FIBBG000B9XRY4']['Close'].to_list()\n",
    "prices_df = pd.DataFrame({'Close': prices})\n",
    "prices_df['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edfe24a7-988e-498c-a302-4c00e6e8d9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "returns_df = prices_df['Close'].pct_change()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52659780-1e10-4e4b-9fa7-620f8c94a78b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           NaN\n",
      "1     -0.012692\n",
      "2     -0.026600\n",
      "3     -0.016693\n",
      "4      0.000989\n",
      "         ...   \n",
      "246   -0.001539\n",
      "247   -0.013879\n",
      "248   -0.030685\n",
      "249    0.028324\n",
      "250    0.002469\n",
      "Name: Close, Length: 251, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(returns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1ad9a63-d4aa-4db1-a1d6-b8907703d2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_returns(returns):\n",
    "    if returns > 0.01:\n",
    "        return 1\n",
    "    elif returns > -0.01:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6efd6c5-8fb3-4305-9662-ff4392a3f325",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Close  Mapped_Returns\n",
      "0    179.4980              -1\n",
      "1    177.2199              -1\n",
      "2    172.5058              -1\n",
      "3    169.6261              -1\n",
      "4    169.7938               0\n",
      "..        ...             ...\n",
      "246  130.9600               0\n",
      "247  129.1424              -1\n",
      "248  125.1797              -1\n",
      "249  128.7253               1\n",
      "250  129.0431               0\n",
      "\n",
      "[251 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "mapped_returns = returns_df.apply(map_returns)\n",
    "\n",
    "prices_df['Mapped_Returns'] = mapped_returns\n",
    "\n",
    "print(prices_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dcacdd22-0fe6-493a-b41f-9beefcf72d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio Values:\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, -1, -1, -1, -1, -1, -2, -2, -2, -1, -1, -1, -1, -2, -2, -2, -1, -1, -2, -2, -2, -1, -1, -2, -2, -3, -3, -3, -3, -3, -4, -4, -4, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -4, -4, -3, -3, -2, -2, -2, -1, -1, -2, -2, -1, -1, -1, -1, -2, -2, -2, -2, -2, -2, -1, -1, -1, -2, -2, -3, -3, -2, -2, -2, -2, -1, -1, -1, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -1, -1, -1, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 8, 8, 8, 8, 8, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 6, 6, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "portfolio_value = 0\n",
    "\n",
    "\n",
    "portfolio_values = []\n",
    "\n",
    "for i in range(0,len(mapped_returns)-1):\n",
    "    # Apply value function\n",
    "    if mapped_returns[i] == 0 and mapped_returns[i+1]==1:\n",
    "        portfolio_value += 1\n",
    "        # this refers to buy action\n",
    "    elif mapped_returns[i] == 0 and mapped_returns[i+1]==-1:\n",
    "        portfolio_value -= 1\n",
    "        #this tells to sell\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    portfolio_values.append(portfolio_value)\n",
    "\n",
    "# Print portfolio values\n",
    "print(\"Portfolio Values:\")\n",
    "print(portfolio_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5401ec30-0c1f-49f9-886a-d61e1cd40b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1: Portfolio Value - 0\n",
      "Day 2: Portfolio Value - -1\n",
      "Day 3: Portfolio Value - -2\n",
      "Day 4: Portfolio Value - -3\n",
      "Day 5: Portfolio Value - -3\n",
      "Day 6: Portfolio Value - -2\n",
      "Day 7: Portfolio Value - -1\n",
      "Day 8: Portfolio Value - 0\n",
      "Day 9: Portfolio Value - -1\n",
      "Day 10: Portfolio Value - -2\n",
      "Day 11: Portfolio Value - -3\n",
      "Day 12: Portfolio Value - -4\n",
      "Day 13: Portfolio Value - -5\n",
      "Day 14: Portfolio Value - -6\n",
      "Day 15: Portfolio Value - -7\n",
      "Day 16: Portfolio Value - -8\n",
      "Day 17: Portfolio Value - -7\n",
      "Day 18: Portfolio Value - -8\n",
      "Day 19: Portfolio Value - -7\n",
      "Day 20: Portfolio Value - -6\n",
      "Day 21: Portfolio Value - -5\n",
      "Day 22: Portfolio Value - -6\n",
      "Day 23: Portfolio Value - -7\n",
      "Day 24: Portfolio Value - -8\n",
      "Day 25: Portfolio Value - -9\n",
      "Day 26: Portfolio Value - -8\n",
      "Day 27: Portfolio Value - -9\n",
      "Day 28: Portfolio Value - -10\n",
      "Day 29: Portfolio Value - -11\n",
      "Day 30: Portfolio Value - -10\n",
      "Day 31: Portfolio Value - -11\n",
      "Day 32: Portfolio Value - -12\n",
      "Day 33: Portfolio Value - -13\n",
      "Day 34: Portfolio Value - -14\n",
      "Day 35: Portfolio Value - -15\n",
      "Day 36: Portfolio Value - -16\n",
      "Day 37: Portfolio Value - -15\n",
      "Day 38: Portfolio Value - -14\n",
      "Day 39: Portfolio Value - -13\n",
      "Day 40: Portfolio Value - -14\n",
      "Day 41: Portfolio Value - -15\n",
      "Day 42: Portfolio Value - -16\n",
      "Day 43: Portfolio Value - -17\n",
      "Day 44: Portfolio Value - -18\n",
      "Day 45: Portfolio Value - -19\n",
      "Day 46: Portfolio Value - -20\n",
      "Day 47: Portfolio Value - -21\n",
      "Day 48: Portfolio Value - -22\n",
      "Day 49: Portfolio Value - -23\n",
      "Day 50: Portfolio Value - -24\n",
      "Day 51: Portfolio Value - -23\n",
      "Day 52: Portfolio Value - -22\n",
      "Day 53: Portfolio Value - -21\n",
      "Day 54: Portfolio Value - -20\n",
      "Day 55: Portfolio Value - -19\n",
      "Day 56: Portfolio Value - -18\n",
      "Day 57: Portfolio Value - -17\n",
      "Day 58: Portfolio Value - -16\n",
      "Day 59: Portfolio Value - -15\n",
      "Day 60: Portfolio Value - -14\n",
      "Day 61: Portfolio Value - -15\n",
      "Day 62: Portfolio Value - -16\n",
      "Day 63: Portfolio Value - -17\n",
      "Day 64: Portfolio Value - -16\n",
      "Day 65: Portfolio Value - -17\n",
      "Day 66: Portfolio Value - -18\n",
      "Day 67: Portfolio Value - -17\n",
      "Day 68: Portfolio Value - -18\n",
      "Day 69: Portfolio Value - -19\n",
      "Day 70: Portfolio Value - -18\n",
      "Day 71: Portfolio Value - -17\n",
      "Day 72: Portfolio Value - -18\n",
      "Day 73: Portfolio Value - -19\n",
      "Day 74: Portfolio Value - -18\n",
      "Day 75: Portfolio Value - -19\n",
      "Day 76: Portfolio Value - -20\n",
      "Day 77: Portfolio Value - -21\n",
      "Day 78: Portfolio Value - -20\n",
      "Day 79: Portfolio Value - -21\n",
      "Day 80: Portfolio Value - -22\n",
      "Day 81: Portfolio Value - -21\n",
      "Day 82: Portfolio Value - -22\n",
      "Day 83: Portfolio Value - -21\n",
      "Day 84: Portfolio Value - -20\n",
      "Day 85: Portfolio Value - -19\n",
      "Day 86: Portfolio Value - -20\n",
      "Day 87: Portfolio Value - -19\n",
      "Day 88: Portfolio Value - -20\n",
      "Day 89: Portfolio Value - -19\n",
      "Day 90: Portfolio Value - -20\n",
      "Day 91: Portfolio Value - -21\n",
      "Day 92: Portfolio Value - -20\n",
      "Day 93: Portfolio Value - -21\n",
      "Day 94: Portfolio Value - -22\n",
      "Day 95: Portfolio Value - -23\n",
      "Day 96: Portfolio Value - -24\n",
      "Day 97: Portfolio Value - -23\n",
      "Day 98: Portfolio Value - -24\n",
      "Day 99: Portfolio Value - -25\n",
      "Day 100: Portfolio Value - -24\n",
      "Day 101: Portfolio Value - -23\n",
      "Day 102: Portfolio Value - -22\n",
      "Day 103: Portfolio Value - -23\n",
      "Day 104: Portfolio Value - -22\n",
      "Day 105: Portfolio Value - -23\n",
      "Day 106: Portfolio Value - -24\n",
      "Day 107: Portfolio Value - -23\n",
      "Day 108: Portfolio Value - -24\n",
      "Day 109: Portfolio Value - -25\n",
      "Day 110: Portfolio Value - -26\n",
      "Day 111: Portfolio Value - -27\n",
      "Day 112: Portfolio Value - -28\n",
      "Day 113: Portfolio Value - -27\n",
      "Day 114: Portfolio Value - -28\n",
      "Day 115: Portfolio Value - -29\n",
      "Day 116: Portfolio Value - -28\n",
      "Day 117: Portfolio Value - -29\n",
      "Day 118: Portfolio Value - -30\n",
      "Day 119: Portfolio Value - -29\n",
      "Day 120: Portfolio Value - -28\n",
      "Day 121: Portfolio Value - -27\n",
      "Day 122: Portfolio Value - -28\n",
      "Day 123: Portfolio Value - -29\n",
      "Day 124: Portfolio Value - -30\n",
      "Day 125: Portfolio Value - -29\n",
      "Day 126: Portfolio Value - -28\n",
      "Day 127: Portfolio Value - -27\n",
      "Day 128: Portfolio Value - -26\n",
      "Day 129: Portfolio Value - -25\n",
      "Day 130: Portfolio Value - -26\n",
      "Day 131: Portfolio Value - -25\n",
      "Day 132: Portfolio Value - -26\n",
      "Day 133: Portfolio Value - -25\n",
      "Day 134: Portfolio Value - -24\n",
      "Day 135: Portfolio Value - -25\n",
      "Day 136: Portfolio Value - -24\n",
      "Day 137: Portfolio Value - -23\n",
      "Day 138: Portfolio Value - -22\n",
      "Day 139: Portfolio Value - -23\n",
      "Day 140: Portfolio Value - -24\n",
      "Day 141: Portfolio Value - -25\n",
      "Day 142: Portfolio Value - -24\n",
      "Day 143: Portfolio Value - -23\n",
      "Day 144: Portfolio Value - -22\n",
      "Day 145: Portfolio Value - -23\n",
      "Day 146: Portfolio Value - -24\n",
      "Day 147: Portfolio Value - -23\n",
      "Day 148: Portfolio Value - -24\n",
      "Day 149: Portfolio Value - -25\n",
      "Day 150: Portfolio Value - -26\n",
      "Day 151: Portfolio Value - -25\n",
      "Day 152: Portfolio Value - -24\n",
      "Day 153: Portfolio Value - -25\n",
      "Day 154: Portfolio Value - -24\n",
      "Day 155: Portfolio Value - -23\n",
      "Day 156: Portfolio Value - -22\n",
      "Day 157: Portfolio Value - -21\n",
      "Day 158: Portfolio Value - -22\n",
      "Day 159: Portfolio Value - -23\n",
      "Day 160: Portfolio Value - -24\n",
      "Day 161: Portfolio Value - -25\n",
      "Day 162: Portfolio Value - -24\n",
      "Day 163: Portfolio Value - -23\n",
      "Day 164: Portfolio Value - -24\n",
      "Day 165: Portfolio Value - -25\n",
      "Day 166: Portfolio Value - -26\n",
      "Day 167: Portfolio Value - -27\n",
      "Day 168: Portfolio Value - -26\n",
      "Day 169: Portfolio Value - -27\n",
      "Day 170: Portfolio Value - -28\n",
      "Day 171: Portfolio Value - -29\n",
      "Day 172: Portfolio Value - -30\n",
      "Day 173: Portfolio Value - -29\n",
      "Day 174: Portfolio Value - -30\n",
      "Day 175: Portfolio Value - -31\n",
      "Day 176: Portfolio Value - -32\n",
      "Day 177: Portfolio Value - -33\n",
      "Day 178: Portfolio Value - -34\n",
      "Day 179: Portfolio Value - -35\n",
      "Day 180: Portfolio Value - -36\n",
      "Day 181: Portfolio Value - -37\n",
      "Day 182: Portfolio Value - -38\n",
      "Day 183: Portfolio Value - -39\n",
      "Day 184: Portfolio Value - -40\n",
      "Day 185: Portfolio Value - -41\n",
      "Day 186: Portfolio Value - -42\n",
      "Day 187: Portfolio Value - -43\n",
      "Day 188: Portfolio Value - -44\n",
      "Day 189: Portfolio Value - -45\n",
      "Day 190: Portfolio Value - -44\n",
      "Day 191: Portfolio Value - -45\n",
      "Day 192: Portfolio Value - -46\n",
      "Day 193: Portfolio Value - -47\n",
      "Day 194: Portfolio Value - -48\n",
      "Day 195: Portfolio Value - -49\n",
      "Day 196: Portfolio Value - -50\n",
      "Day 197: Portfolio Value - -51\n",
      "Day 198: Portfolio Value - -52\n",
      "Day 199: Portfolio Value - -53\n",
      "Day 200: Portfolio Value - -54\n",
      "Day 201: Portfolio Value - -53\n",
      "Day 202: Portfolio Value - -54\n",
      "Day 203: Portfolio Value - -55\n",
      "Day 204: Portfolio Value - -54\n",
      "Day 205: Portfolio Value - -55\n",
      "Day 206: Portfolio Value - -56\n",
      "Day 207: Portfolio Value - -57\n",
      "Day 208: Portfolio Value - -58\n",
      "Day 209: Portfolio Value - -59\n",
      "Day 210: Portfolio Value - -60\n",
      "Day 211: Portfolio Value - -61\n",
      "Day 212: Portfolio Value - -62\n",
      "Day 213: Portfolio Value - -63\n",
      "Day 214: Portfolio Value - -64\n",
      "Day 215: Portfolio Value - -65\n",
      "Day 216: Portfolio Value - -66\n",
      "Day 217: Portfolio Value - -67\n",
      "Day 218: Portfolio Value - -68\n",
      "Day 219: Portfolio Value - -69\n",
      "Day 220: Portfolio Value - -70\n",
      "Day 221: Portfolio Value - -71\n",
      "Day 222: Portfolio Value - -72\n",
      "Day 223: Portfolio Value - -73\n",
      "Day 224: Portfolio Value - -74\n",
      "Day 225: Portfolio Value - -75\n",
      "Day 226: Portfolio Value - -76\n",
      "Day 227: Portfolio Value - -77\n",
      "Day 228: Portfolio Value - -78\n",
      "Day 229: Portfolio Value - -79\n",
      "Day 230: Portfolio Value - -80\n",
      "Day 231: Portfolio Value - -81\n",
      "Day 232: Portfolio Value - -82\n",
      "Day 233: Portfolio Value - -83\n",
      "Day 234: Portfolio Value - -84\n",
      "Day 235: Portfolio Value - -85\n",
      "Day 236: Portfolio Value - -86\n",
      "Day 237: Portfolio Value - -87\n",
      "Day 238: Portfolio Value - -88\n",
      "Day 239: Portfolio Value - -89\n",
      "Day 240: Portfolio Value - -90\n",
      "Day 241: Portfolio Value - -91\n",
      "Day 242: Portfolio Value - -92\n",
      "Day 243: Portfolio Value - -93\n",
      "Day 244: Portfolio Value - -92\n",
      "Day 245: Portfolio Value - -93\n",
      "Day 246: Portfolio Value - -94\n",
      "Day 247: Portfolio Value - -95\n",
      "Day 248: Portfolio Value - -96\n",
      "Day 249: Portfolio Value - -97\n",
      "Day 250: Portfolio Value - -98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Greedy Approach \n",
    "class PortfolioOptimizer:\n",
    "    def __init__(self):\n",
    "        self.portfolio_value = 0\n",
    "        self.transition_counts = {'+1': {'+1': 0, '0': 0, '-1': 0},\n",
    "                                  '0': {'+1': 0, '0': 0, '-1': 0},\n",
    "                                  '-1': {'+1': 0, '0': 0, '-1': 0}}\n",
    "        self.current_state = None\n",
    "        self.transition_probabilities = None\n",
    "\n",
    "    def classify_state(self, returns):\n",
    "        if returns >= 0.1:\n",
    "            return '+1'\n",
    "        elif returns > -0.1:\n",
    "            return '0'\n",
    "        else:\n",
    "            return '-1'\n",
    "\n",
    "    def update_transition_counts(self, previous_state, current_state):\n",
    "        self.transition_counts[previous_state][current_state] += 1\n",
    "        # Set transition probabilities to None to indicate they need to be recalculated\n",
    "        self.transition_probabilities = None\n",
    "\n",
    "    def calculate_transition_probabilities(self):\n",
    "        if self.transition_probabilities is None:\n",
    "            transition_probabilities = {}\n",
    "            for previous_state, transitions in self.transition_counts.items():\n",
    "                total_transitions = sum(transitions.values())\n",
    "                transition_probabilities[previous_state] = {}\n",
    "                for new_state, count in transitions.items():\n",
    "                    if total_transitions != 0:\n",
    "                        transition_probabilities[previous_state][new_state] = count / total_transitions\n",
    "                    else:\n",
    "                        transition_probabilities[previous_state][new_state] = 0\n",
    "            self.transition_probabilities = transition_probabilities\n",
    "        return self.transition_probabilities\n",
    "    \n",
    "    def make_decision(self, current_returns):\n",
    "        current_state = self.classify_state(current_returns)\n",
    "        if self.current_state is not None:\n",
    "            self.update_transition_counts(self.current_state, current_state)\n",
    "\n",
    "            # Calculate transition probabilities only when necessary\n",
    "            transition_probabilities = self.calculate_transition_probabilities()\n",
    "\n",
    "            # Find the action with the highest transition probability\n",
    "            max_probability_action = max(transition_probabilities[self.current_state], key=transition_probabilities[self.current_state].get)\n",
    "\n",
    "            # Perform the action based on the highest transition probability\n",
    "            if max_probability_action == '+1':\n",
    "                #print(\"The probability of going to state +1 is greater than other states\")\n",
    "                self.portfolio_value += 1\n",
    "            elif max_probability_action == '-1':\n",
    "                #print(\"The probability of going to state -1 is greater than other states\")\n",
    "                self.portfolio_value -= 1\n",
    "            else:\n",
    "                #print(\"The probability of staying in state 0 is greater than other states\")\n",
    "                yy=0\n",
    "\n",
    "        self.current_state = current_state\n",
    "\n",
    "\n",
    "# Example usage\n",
    "portfolio_optimizer = PortfolioOptimizer()\n",
    "\n",
    "# Iterate over each day's returns and make decisions\n",
    "for i in range(1, len(prices)):\n",
    "    current_return = ((prices[i] - prices[i-1]) / prices[i-1])*100  # Calculate the return for the current day\n",
    "    portfolio_optimizer.make_decision(current_return)\n",
    "    print(f\"Day {i}: Portfolio Value - {portfolio_optimizer.portfolio_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "97d3cd97-2563-4abd-8cd7-800d8b004901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1: Portfolio Value - 0\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 2: Portfolio Value - -1\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 3: Portfolio Value - -2\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 4: Portfolio Value - -3\n",
      "The probability of staying in state 0 is greater than other states\n",
      "Day 5: Portfolio Value - -3\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 6: Portfolio Value - -2\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 7: Portfolio Value - -1\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 8: Portfolio Value - 0\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 9: Portfolio Value - -1\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 10: Portfolio Value - -2\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 11: Portfolio Value - -3\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 12: Portfolio Value - -4\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 13: Portfolio Value - -5\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 14: Portfolio Value - -6\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 15: Portfolio Value - -7\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 16: Portfolio Value - -8\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 17: Portfolio Value - -7\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 18: Portfolio Value - -8\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 19: Portfolio Value - -7\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 20: Portfolio Value - -6\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 21: Portfolio Value - -5\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 22: Portfolio Value - -6\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 23: Portfolio Value - -7\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 24: Portfolio Value - -8\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 25: Portfolio Value - -9\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 26: Portfolio Value - -8\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 27: Portfolio Value - -9\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 28: Portfolio Value - -10\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 29: Portfolio Value - -11\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 30: Portfolio Value - -10\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 31: Portfolio Value - -11\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 32: Portfolio Value - -12\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 33: Portfolio Value - -13\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 34: Portfolio Value - -14\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 35: Portfolio Value - -15\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 36: Portfolio Value - -16\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 37: Portfolio Value - -15\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 38: Portfolio Value - -14\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 39: Portfolio Value - -13\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 40: Portfolio Value - -14\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 41: Portfolio Value - -15\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 42: Portfolio Value - -16\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 43: Portfolio Value - -17\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 44: Portfolio Value - -18\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 45: Portfolio Value - -19\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 46: Portfolio Value - -20\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 47: Portfolio Value - -21\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 48: Portfolio Value - -22\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 49: Portfolio Value - -23\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 50: Portfolio Value - -24\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 51: Portfolio Value - -23\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 52: Portfolio Value - -22\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 53: Portfolio Value - -21\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 54: Portfolio Value - -20\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 55: Portfolio Value - -19\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 56: Portfolio Value - -18\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 57: Portfolio Value - -17\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 58: Portfolio Value - -16\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 59: Portfolio Value - -15\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 60: Portfolio Value - -14\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 61: Portfolio Value - -15\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 62: Portfolio Value - -16\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 63: Portfolio Value - -17\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 64: Portfolio Value - -16\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 65: Portfolio Value - -17\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 66: Portfolio Value - -18\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 67: Portfolio Value - -17\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 68: Portfolio Value - -18\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 69: Portfolio Value - -19\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 70: Portfolio Value - -18\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 71: Portfolio Value - -17\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 72: Portfolio Value - -18\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 73: Portfolio Value - -19\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 74: Portfolio Value - -18\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 75: Portfolio Value - -19\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 76: Portfolio Value - -20\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 77: Portfolio Value - -21\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 78: Portfolio Value - -20\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 79: Portfolio Value - -21\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 80: Portfolio Value - -22\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 81: Portfolio Value - -21\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 82: Portfolio Value - -22\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 83: Portfolio Value - -21\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 84: Portfolio Value - -20\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 85: Portfolio Value - -19\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 86: Portfolio Value - -20\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 87: Portfolio Value - -19\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 88: Portfolio Value - -20\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 89: Portfolio Value - -19\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 90: Portfolio Value - -20\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 91: Portfolio Value - -21\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 92: Portfolio Value - -20\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 93: Portfolio Value - -21\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 94: Portfolio Value - -22\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 95: Portfolio Value - -23\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 96: Portfolio Value - -24\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 97: Portfolio Value - -23\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 98: Portfolio Value - -24\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 99: Portfolio Value - -25\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 100: Portfolio Value - -24\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 101: Portfolio Value - -23\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 102: Portfolio Value - -22\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 103: Portfolio Value - -23\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 104: Portfolio Value - -22\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 105: Portfolio Value - -23\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 106: Portfolio Value - -24\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 107: Portfolio Value - -23\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 108: Portfolio Value - -24\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 109: Portfolio Value - -25\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 110: Portfolio Value - -26\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 111: Portfolio Value - -27\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 112: Portfolio Value - -28\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 113: Portfolio Value - -27\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 114: Portfolio Value - -28\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 115: Portfolio Value - -29\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 116: Portfolio Value - -28\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 117: Portfolio Value - -29\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 118: Portfolio Value - -30\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 119: Portfolio Value - -29\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 120: Portfolio Value - -28\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 121: Portfolio Value - -27\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 122: Portfolio Value - -28\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 123: Portfolio Value - -29\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 124: Portfolio Value - -30\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 125: Portfolio Value - -29\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 126: Portfolio Value - -28\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 127: Portfolio Value - -27\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 128: Portfolio Value - -26\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 129: Portfolio Value - -25\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 130: Portfolio Value - -26\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 131: Portfolio Value - -25\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 132: Portfolio Value - -26\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 133: Portfolio Value - -25\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 134: Portfolio Value - -24\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 135: Portfolio Value - -25\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 136: Portfolio Value - -24\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 137: Portfolio Value - -23\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 138: Portfolio Value - -22\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 139: Portfolio Value - -23\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 140: Portfolio Value - -24\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 141: Portfolio Value - -25\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 142: Portfolio Value - -24\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 143: Portfolio Value - -23\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 144: Portfolio Value - -22\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 145: Portfolio Value - -23\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 146: Portfolio Value - -24\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 147: Portfolio Value - -23\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 148: Portfolio Value - -24\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 149: Portfolio Value - -25\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 150: Portfolio Value - -26\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 151: Portfolio Value - -25\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 152: Portfolio Value - -24\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 153: Portfolio Value - -25\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 154: Portfolio Value - -24\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 155: Portfolio Value - -23\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 156: Portfolio Value - -22\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 157: Portfolio Value - -21\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 158: Portfolio Value - -22\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 159: Portfolio Value - -23\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 160: Portfolio Value - -24\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 161: Portfolio Value - -25\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 162: Portfolio Value - -24\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 163: Portfolio Value - -23\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 164: Portfolio Value - -24\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 165: Portfolio Value - -25\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 166: Portfolio Value - -26\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 167: Portfolio Value - -27\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 168: Portfolio Value - -26\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 169: Portfolio Value - -27\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 170: Portfolio Value - -28\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 171: Portfolio Value - -29\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 172: Portfolio Value - -30\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 173: Portfolio Value - -29\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 174: Portfolio Value - -30\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 175: Portfolio Value - -31\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 176: Portfolio Value - -32\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 177: Portfolio Value - -33\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 178: Portfolio Value - -34\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 179: Portfolio Value - -35\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 180: Portfolio Value - -36\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 181: Portfolio Value - -37\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 182: Portfolio Value - -38\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 183: Portfolio Value - -39\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 184: Portfolio Value - -40\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 185: Portfolio Value - -41\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 186: Portfolio Value - -42\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 187: Portfolio Value - -43\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 188: Portfolio Value - -44\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 189: Portfolio Value - -45\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 190: Portfolio Value - -44\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 191: Portfolio Value - -45\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 192: Portfolio Value - -46\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 193: Portfolio Value - -47\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 194: Portfolio Value - -48\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 195: Portfolio Value - -49\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 196: Portfolio Value - -50\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 197: Portfolio Value - -51\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 198: Portfolio Value - -52\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 199: Portfolio Value - -53\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 200: Portfolio Value - -54\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 201: Portfolio Value - -53\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 202: Portfolio Value - -54\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 203: Portfolio Value - -55\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 204: Portfolio Value - -54\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 205: Portfolio Value - -55\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 206: Portfolio Value - -56\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 207: Portfolio Value - -57\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 208: Portfolio Value - -58\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 209: Portfolio Value - -59\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 210: Portfolio Value - -60\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 211: Portfolio Value - -61\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 212: Portfolio Value - -62\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 213: Portfolio Value - -63\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 214: Portfolio Value - -64\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 215: Portfolio Value - -65\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 216: Portfolio Value - -66\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 217: Portfolio Value - -67\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 218: Portfolio Value - -68\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 219: Portfolio Value - -69\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 220: Portfolio Value - -70\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 221: Portfolio Value - -71\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 222: Portfolio Value - -72\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 223: Portfolio Value - -73\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 224: Portfolio Value - -74\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 225: Portfolio Value - -75\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 226: Portfolio Value - -76\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 227: Portfolio Value - -77\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 228: Portfolio Value - -78\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 229: Portfolio Value - -79\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 230: Portfolio Value - -80\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 231: Portfolio Value - -81\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 232: Portfolio Value - -82\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 233: Portfolio Value - -83\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 234: Portfolio Value - -84\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 235: Portfolio Value - -85\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 236: Portfolio Value - -86\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 237: Portfolio Value - -87\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 238: Portfolio Value - -88\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 239: Portfolio Value - -89\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 240: Portfolio Value - -90\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 241: Portfolio Value - -91\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 242: Portfolio Value - -92\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 243: Portfolio Value - -93\n",
      "The probability of going to state +1 is greater than other states\n",
      "Day 244: Portfolio Value - -92\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 245: Portfolio Value - -93\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 246: Portfolio Value - -94\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 247: Portfolio Value - -95\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 248: Portfolio Value - -96\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 249: Portfolio Value - -97\n",
      "The probability of going to state -1 is greater than other states\n",
      "Day 250: Portfolio Value - -98\n",
      "Optimal Indices to Buy the Stock: [6, 7, 8, 17, 19, 20, 21, 26, 30, 37, 38, 39, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 64, 67, 70, 71, 74, 78, 81, 83, 84, 85, 87, 89, 92, 97, 100, 101, 102, 104, 107, 113, 116, 119, 120, 121, 125, 126, 127, 128, 129, 131, 133, 134, 136, 137, 138, 142, 143, 144, 147, 151, 152, 154, 155, 156, 157, 162, 163, 168, 173, 190, 201, 204, 244]\n"
     ]
    }
   ],
   "source": [
    "class PortfolioOptimizer:\n",
    "    def __init__(self):\n",
    "        self.portfolio_value = 0\n",
    "        self.transition_counts = {'+1': {'+1': 0, '0': 0, '-1': 0},\n",
    "                                  '0': {'+1': 0, '0': 0, '-1': 0},\n",
    "                                  '-1': {'+1': 0, '0': 0, '-1': 0}}\n",
    "        self.current_state = None\n",
    "        self.transition_probabilities = None\n",
    "        self.buy_indices = []  # List to store indices of buy decisions\n",
    "\n",
    "    def classify_state(self, returns):\n",
    "        if returns >= 0.1:\n",
    "            return '+1'\n",
    "        elif returns > -0.1:\n",
    "            return '0'\n",
    "        else:\n",
    "            return '-1'\n",
    "\n",
    "    def update_transition_counts(self, previous_state, current_state):\n",
    "        self.transition_counts[previous_state][current_state] += 1\n",
    "        # Set transition probabilities to None to indicate they need to be recalculated\n",
    "        self.transition_probabilities = None\n",
    "\n",
    "    def calculate_transition_probabilities(self):\n",
    "        if self.transition_probabilities is None:\n",
    "            transition_probabilities = {}\n",
    "            for previous_state, transitions in self.transition_counts.items():\n",
    "                total_transitions = sum(transitions.values())\n",
    "                transition_probabilities[previous_state] = {}\n",
    "                for new_state, count in transitions.items():\n",
    "                    if total_transitions != 0:\n",
    "                        transition_probabilities[previous_state][new_state] = count / total_transitions\n",
    "                    else:\n",
    "                        transition_probabilities[previous_state][new_state] = 0\n",
    "            self.transition_probabilities = transition_probabilities\n",
    "        return self.transition_probabilities\n",
    "    \n",
    "    def make_decision(self, current_returns, day_index):\n",
    "        current_state = self.classify_state(current_returns)\n",
    "        if self.current_state is not None:\n",
    "            self.update_transition_counts(self.current_state, current_state)\n",
    "\n",
    "            # Calculate transition probabilities only when necessary\n",
    "            transition_probabilities = self.calculate_transition_probabilities()\n",
    "\n",
    "            # Find the action with the highest transition probability\n",
    "            max_probability_action = max(transition_probabilities[self.current_state], key=transition_probabilities[self.current_state].get)\n",
    "\n",
    "            # Perform the action based on the highest transition probability\n",
    "            if max_probability_action == '+1':\n",
    "                print(\"The probability of going to state +1 is greater than other states\")\n",
    "                self.portfolio_value += 1\n",
    "                self.buy_indices.append(day_index)  # Record index of buy decision\n",
    "            elif max_probability_action == '-1':\n",
    "                print(\"The probability of going to state -1 is greater than other states\")\n",
    "                self.portfolio_value -= 1\n",
    "            else:\n",
    "                print(\"The probability of staying in state 0 is greater than other states\")\n",
    "\n",
    "        self.current_state = current_state\n",
    "\n",
    "\n",
    "# Example usage\n",
    "portfolio_optimizer = PortfolioOptimizer()\n",
    "\n",
    "# Iterate over each day's returns and make decisions\n",
    "for i in range(1, len(prices)):\n",
    "    current_return = ((prices[i] - prices[i-1]) / prices[i-1]) * 100  # Calculate the return for the current day\n",
    "    portfolio_optimizer.make_decision(current_return, i)\n",
    "    print(f\"Day {i}: Portfolio Value - {portfolio_optimizer.portfolio_value}\")\n",
    "\n",
    "# Print the optimal indices to buy the stock\n",
    "print(\"Optimal Indices to Buy the Stock:\", portfolio_optimizer.buy_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f72a3454-a186-4b31-b7d0-85544e9ab784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bear': {'Buy': {'Bear': 0.3333333333333333, 'Bull': 0.3333333333333333, 'Flat': 0.3333333333333333}, 'Sell': {'Bear': 0.3333333333333333, 'Bull': 0.3333333333333333, 'Flat': 0.3333333333333333}, 'Hold': {'Bear': 0.3333333333333333, 'Bull': 0.3333333333333333, 'Flat': 0.3333333333333333}}, 'Bull': {'Buy': {'Bear': 0.3333333333333333, 'Bull': 0.3333333333333333, 'Flat': 0.3333333333333333}, 'Sell': {'Bear': 0.3333333333333333, 'Bull': 0.3333333333333333, 'Flat': 0.3333333333333333}, 'Hold': {'Bear': 0.3333333333333333, 'Bull': 0.3333333333333333, 'Flat': 0.3333333333333333}}, 'Flat': {'Buy': {'Bear': 0.3333333333333333, 'Bull': 0.3333333333333333, 'Flat': 0.3333333333333333}, 'Sell': {'Bear': 0.3333333333333333, 'Bull': 0.3333333333333333, 'Flat': 0.3333333333333333}, 'Hold': {'Bear': 0.3333333333333333, 'Bull': 0.3333333333333333, 'Flat': 0.3333333333333333}}}\n",
      "Iteration 1:\n",
      "State: Bear, Optimal Action: Buy, Value: 1.6666666666666665\n",
      "State: Bull, Optimal Action: Sell, Value: 3.7777777777777777\n",
      "State: Flat, Optimal Action: Buy, Value: 3.1185185185185182\n",
      "Iteration 2:\n",
      "State: Bear, Optimal Action: Buy, Value: 3.9501234567901236\n",
      "State: Bull, Optimal Action: Sell, Value: 6.225711934156378\n",
      "State: Flat, Optimal Action: Buy, Value: 5.211827709190672\n",
      "Iteration 3:\n",
      "State: Bear, Optimal Action: Buy, Value: 5.770043493369913\n",
      "State: Bull, Optimal Action: Sell, Value: 7.922022169791189\n",
      "State: Flat, Optimal Action: Buy, Value: 6.707704899293807\n",
      "Iteration 4:\n",
      "State: Bear, Optimal Action: Buy, Value: 7.106605483321308\n",
      "State: Bull, Optimal Action: Sell, Value: 9.12968868064168\n",
      "State: Flat, Optimal Action: Buy, Value: 7.785066416868479\n",
      "Iteration 5:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.072362821555057\n",
      "State: Bull, Optimal Action: Sell, Value: 9.99656477841739\n",
      "State: Flat, Optimal Action: Buy, Value: 8.56106507115758\n",
      "Iteration 6:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.767998045634673\n",
      "State: Bull, Optimal Action: Sell, Value: 10.620167438722572\n",
      "State: Flat, Optimal Action: Buy, Value: 9.119794814803953\n",
      "Iteration 7:\n",
      "State: Bear, Optimal Action: Buy, Value: 9.268789413109653\n",
      "State: Bull, Optimal Action: Sell, Value: 11.069000444436314\n",
      "State: Flat, Optimal Action: Buy, Value: 9.522022579293312\n",
      "Iteration 8:\n",
      "State: Bear, Optimal Action: Buy, Value: 9.629283316490476\n",
      "State: Bull, Optimal Action: Sell, Value: 11.39208169072536\n",
      "State: Flat, Optimal Action: Buy, Value: 9.811570023069105\n",
      "Iteration 9:\n",
      "State: Bear, Optimal Action: Buy, Value: 9.88878267474265\n",
      "State: Bull, Optimal Action: Sell, Value: 11.624649170276564\n",
      "State: Flat, Optimal Action: Buy, Value: 10.020000498156886\n",
      "Iteration 10:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.075581958180292\n",
      "State: Bull, Optimal Action: Sell, Value: 11.792061767096998\n",
      "State: Flat, Optimal Action: Buy, Value: 10.170038459582447\n",
      "Iteration 11:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.210048582629263\n",
      "State: Bull, Optimal Action: Sell, Value: 11.912573015815655\n",
      "State: Flat, Optimal Action: Buy, Value: 10.27804268214063\n",
      "Iteration 12:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.306843808156145\n",
      "State: Bull, Optimal Action: Sell, Value: 11.999322534963314\n",
      "State: Flat, Optimal Action: Buy, Value: 10.35578907340269\n",
      "Iteration 13:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.376521444405906\n",
      "State: Bull, Optimal Action: Sell, Value: 12.06176881407251\n",
      "State: Flat, Optimal Action: Buy, Value: 10.411754488501629\n",
      "Iteration 14:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.426678599194679\n",
      "State: Bull, Optimal Action: Sell, Value: 12.106720507138352\n",
      "State: Flat, Optimal Action: Buy, Value: 10.452040958622575\n",
      "Iteration 15:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.462784017321495\n",
      "State: Bull, Optimal Action: Sell, Value: 12.139078795488645\n",
      "State: Flat, Optimal Action: Buy, Value: 10.48104100571539\n",
      "Iteration 16:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.488774351606809\n",
      "State: Bull, Optimal Action: Sell, Value: 12.16237177408289\n",
      "State: Flat, Optimal Action: Buy, Value: 10.50191656837469\n",
      "Iteration 17:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.507483385083837\n",
      "State: Bull, Optimal Action: Sell, Value: 12.179139127344378\n",
      "State: Flat, Optimal Action: Buy, Value: 10.516943754880774\n",
      "Iteration 18:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.52095100461573\n",
      "State: Bull, Optimal Action: Sell, Value: 12.191209036490902\n",
      "State: Flat, Optimal Action: Buy, Value: 10.527761012263309\n",
      "Iteration 19:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.530645614231982\n",
      "State: Bull, Optimal Action: Sell, Value: 12.19989751012965\n",
      "State: Flat, Optimal Action: Buy, Value: 10.535547769766652\n",
      "Iteration 20:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.537624238434208\n",
      "State: Bull, Optimal Action: Sell, Value: 12.206151871554804\n",
      "State: Flat, Optimal Action: Buy, Value: 10.54115303460151\n",
      "Iteration 21:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.542647771890804\n",
      "State: Bull, Optimal Action: Sell, Value: 12.210654047479231\n",
      "State: Flat, Optimal Action: Buy, Value: 10.545187961059078\n",
      "Iteration 22:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.546263941447764\n",
      "State: Bull, Optimal Action: Sell, Value: 12.213894919996285\n",
      "State: Flat, Optimal Action: Buy, Value: 10.548092486000835\n",
      "Iteration 23:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.548867025985302\n",
      "State: Bull, Optimal Action: Sell, Value: 12.216227848528646\n",
      "State: Flat, Optimal Action: Buy, Value: 10.550183296137275\n",
      "Iteration 24:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.550740845506994\n",
      "State: Bull, Optimal Action: Sell, Value: 12.217907197379443\n",
      "State: Flat, Optimal Action: Buy, Value: 10.55168835707299\n",
      "Iteration 25:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.552089706655845\n",
      "State: Bull, Optimal Action: Sell, Value: 12.219116069628871\n",
      "State: Flat, Optimal Action: Buy, Value: 10.552771768895388\n",
      "Iteration 26:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.553060678714695\n",
      "State: Bull, Optimal Action: Sell, Value: 12.219986271263721\n",
      "State: Flat, Optimal Action: Buy, Value: 10.553551658366349\n",
      "Iteration 27:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.553759628891939\n",
      "State: Bull, Optimal Action: Sell, Value: 12.220612682272534\n",
      "State: Flat, Optimal Action: Buy, Value: 10.554113058541553\n",
      "Iteration 28:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.55426276525494\n",
      "State: Bull, Optimal Action: Sell, Value: 12.221063601618408\n",
      "State: Flat, Optimal Action: Buy, Value: 10.55451718011064\n",
      "Iteration 29:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.554624945862397\n",
      "State: Bull, Optimal Action: Sell, Value: 12.221388194024385\n",
      "State: Flat, Optimal Action: Buy, Value: 10.554808085332645\n",
      "Iteration 30:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.554885660058513\n",
      "State: Bull, Optimal Action: Sell, Value: 12.221621850510811\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555017492240523\n",
      "Iteration 31:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555073334082625\n",
      "State: Bull, Optimal Action: Sell, Value: 12.221790047155721\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555168232927699\n",
      "Iteration 32:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555208430444278\n",
      "State: Bull, Optimal Action: Sell, Value: 12.221911122807388\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555276742981164\n",
      "Iteration 33:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.55530567899542\n",
      "State: Bull, Optimal Action: Sell, Value: 12.221998278609059\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555354853489504\n",
      "Iteration 34:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555375682958395\n",
      "State: Bull, Optimal Action: Sell, Value: 12.222061017348523\n",
      "State: Flat, Optimal Action: Buy, Value: 10.55541108101238\n",
      "Iteration 35:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555426075018481\n",
      "State: Bull, Optimal Action: Sell, Value: 12.222106179567835\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555451556159651\n",
      "Iteration 36:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555462349532256\n",
      "State: Bull, Optimal Action: Sell, Value: 12.222138689402598\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555480692025203\n",
      "Iteration 37:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555488461589347\n",
      "State: Bull, Optimal Action: Sell, Value: 12.22216209147124\n",
      "State: Flat, Optimal Action: Buy, Value: 10.55550166535621\n",
      "Iteration 38:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555507258244479\n",
      "State: Bull, Optimal Action: Sell, Value: 12.222178937352513\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555516762920854\n",
      "Iteration 39:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555520788938093\n",
      "State: Bull, Optimal Action: Sell, Value: 12.222191063789722\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555527630839645\n",
      "Iteration 40:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555530528951323\n",
      "State: Bull, Optimal Action: Sell, Value: 12.22219979295485\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555535454065552\n",
      "Iteration 41:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555537540259126\n",
      "State: Bull, Optimal Action: Sell, Value: 12.222206076607874\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555541085582014\n",
      "Iteration 42:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555542587319735\n",
      "State: Bull, Optimal Action: Sell, Value: 12.222210599869232\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555545139405595\n",
      "Iteration 43:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555546220425216\n",
      "State: Bull, Optimal Action: Sell, Value: 12.22221385592001\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555548057533553\n",
      "Iteration 44:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555548835701007\n",
      "State: Bull, Optimal Action: Sell, Value: 12.222216199774552\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555550158135762\n",
      "Iteration 45:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555550718296352\n",
      "State: Bull, Optimal Action: Sell, Value: 12.222217886988442\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555551670245482\n",
      "Iteration 46:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555552073474741\n",
      "State: Bull, Optimal Action: Sell, Value: 12.22221910152231\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555552758731341\n",
      "Iteration 47:\n",
      "State: Bear, Optimal Action: Buy, Value: 10.555553048994238\n",
      "State: Bull, Optimal Action: Sell, Value: 12.222219975799437\n",
      "State: Flat, Optimal Action: Buy, Value: 10.555553542273337\n",
      "Optimal Policy:\n",
      "State: Bear, Optimal Action: Buy\n",
      "State: Bull, Optimal Action: Sell\n",
      "State: Flat, Optimal Action: Buy\n",
      "{'Bear': 10.555553048994238, 'Bull': 12.222219975799437, 'Flat': 10.555553542273337}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define states and actions\n",
    "states = ['Bear', 'Bull', 'Flat']\n",
    "actions = ['Buy', 'Sell', 'Hold']\n",
    "\n",
    "# Considering Uniform transition probability \n",
    "\n",
    "# Define rewards for each state-action pair\n",
    "rewards = {\n",
    "    'Bear': {\n",
    "        'Buy': {'Bear': 10, 'Bull': -5, 'Flat': 0},\n",
    "        'Sell': {'Bear': -5, 'Bull': 0, 'Flat': 0},\n",
    "        'Hold': {'Bear': 0, 'Bull': 0, 'Flat': 0}\n",
    "    },\n",
    "    'Bull': {\n",
    "        'Buy': {'Bear': 0, 'Bull': 0, 'Flat': 0},\n",
    "        'Sell': {'Bear': 0, 'Bull': 10, 'Flat': 0},\n",
    "        'Hold': {'Bear': 0, 'Bull': 0, 'Flat': 0}\n",
    "    },\n",
    "    'Flat': {\n",
    "        'Buy': {'Bear': 0, 'Bull': 0, 'Flat': 5},\n",
    "        'Sell': {'Bear': 0, 'Bull': 0, 'Flat': 5},\n",
    "        'Hold': {'Bear': 0, 'Bull': 0, 'Flat': 0}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define transition probabilities (assuming uniform distribution)\n",
    "transition_probabilities = {\n",
    "    state: {\n",
    "        action: {next_state: 1/len(states) for next_state in states}\n",
    "        for action in actions\n",
    "    }\n",
    "    for state in states\n",
    "}\n",
    "\n",
    "print(transition_probabilities)\n",
    "# Initialize value function\n",
    "V = {state: 0 for state in states}\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.8\n",
    "\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Perform value iteration until convergence\n",
    "iteration = 0\n",
    "while True:\n",
    "    iteration += 1\n",
    "    delta = 0\n",
    "    print(f\"Iteration {iteration}:\")\n",
    "    for state in states:\n",
    "        old_value = V[state]\n",
    "        max_value = float('-inf')\n",
    "        max_action = None\n",
    "        #Using the Bellmans equation\n",
    "        for action in actions:\n",
    "            action_value = sum(transition_probabilities[state][action][next_state] *\n",
    "                               (rewards[state][action][next_state] + gamma * V[next_state])\n",
    "                               for next_state in states)\n",
    "            if action_value > max_value:\n",
    "                max_value = action_value\n",
    "                max_action = action\n",
    "        V[state] = max_value\n",
    "        delta = max(delta, abs(old_value - V[state]))\n",
    "        print(f\"State: {state}, Optimal Action: {max_action}, Value: {V[state]}\")\n",
    "    if delta < tolerance:\n",
    "        break\n",
    "\n",
    "# Derive optimal policy\n",
    "optimal_policy = {}\n",
    "for state in states:\n",
    "    max_action = None\n",
    "    max_action_value = float('-inf')\n",
    "    for action in actions:\n",
    "        action_value = sum(transition_probabilities[state][action][next_state] *\n",
    "                           (rewards[state][action][next_state] + gamma * V[next_state])\n",
    "                           for next_state in states)\n",
    "        if action_value > max_action_value:\n",
    "            max_action = action\n",
    "            max_action_value = action_value\n",
    "    optimal_policy[state] = max_action\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State: {state}, Optimal Action: {action}\")\n",
    "    \n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "971fdc39-8839-4236-969f-be763173fb91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probability Matrix:\n",
      "[[[0.         0.         0.        ]\n",
      "  [0.4137931  0.34482759 0.24137931]\n",
      "  [0.         0.         0.        ]]\n",
      "\n",
      " [[1.         0.         0.        ]\n",
      "  [0.         1.         0.        ]\n",
      "  [0.         0.         1.        ]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.28169014 0.47887324 0.23943662]\n",
      "  [0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the states, actions, and next states\n",
    "states = [-1, 0, 1]\n",
    "actions = [-1, 0, 1]\n",
    "next_states = [-1, 0, 1]\n",
    "\n",
    "# Initialize the transition probability matrix\n",
    "transition_probabilities = np.zeros((len(states), len(actions), len(next_states)))\n",
    "\n",
    "# Iterate over the transition counts to calculate probabilities\n",
    "for i, state in enumerate(states):\n",
    "    for j, action in enumerate(actions):\n",
    "        total_transitions = sum(transition_counts[(state, action)].values())\n",
    "        for k, next_state in enumerate(next_states):\n",
    "            transition_count = transition_counts[(state, action)].get(next_state, 0)\n",
    "            if total_transitions != 0:\n",
    "                transition_probabilities[i, j, k] = transition_count / total_transitions\n",
    "\n",
    "# Print the transition probability matrix\n",
    "print(\"Transition Probability Matrix:\")\n",
    "print(transition_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6c871a00-4462-4ecd-9fb8-df37cd16dc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probability Matrix:\n",
      "[[[0.         0.         0.        ]\n",
      "  [0.4137931  0.34482759 0.24137931]\n",
      "  [0.         0.         0.        ]]\n",
      "\n",
      " [[1.         0.         0.        ]\n",
      "  [0.         1.         0.        ]\n",
      "  [0.         0.         1.        ]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.28169014 0.47887324 0.23943662]\n",
      "  [0.         0.         0.        ]]]\n",
      "Iteration 1:\n",
      "State: -1, Optimal Action: -1, Value: 0.0\n",
      "State: 0, Optimal Action: -1, Value: 0.0\n",
      "State: 1, Optimal Action: 1, Value: 10.0\n",
      "Iteration 2:\n",
      "State: -1, Optimal Action: 1, Value: 3.8309859154929575\n",
      "State: 0, Optimal Action: 1, Value: 3.498397280233123\n",
      "State: 1, Optimal Action: 1, Value: 18.0\n",
      "Iteration 3:\n",
      "State: -1, Optimal Action: 1, Value: 8.4179685202033\n",
      "State: 0, Optimal Action: 1, Value: 7.749146124254361\n",
      "State: 1, Optimal Action: 1, Value: 24.4\n",
      "Iteration 4:\n",
      "State: -1, Optimal Action: 1, Value: 12.706348026237107\n",
      "State: 0, Optimal Action: 1, Value: 11.749908680681713\n",
      "State: 1, Optimal Action: 1, Value: 29.52\n",
      "Iteration 5:\n",
      "State: -1, Optimal Action: 1, Value: 16.390829183770876\n",
      "State: 0, Optimal Action: 1, Value: 15.198198853919356\n",
      "State: 1, Optimal Action: 1, Value: 33.616\n",
      "Iteration 6:\n",
      "State: -1, Optimal Action: 1, Value: 19.44284026143653\n",
      "State: 0, Optimal Action: 1, Value: 18.058986705574846\n",
      "State: 1, Optimal Action: 1, Value: 36.8928\n",
      "Iteration 7:\n",
      "State: -1, Optimal Action: 1, Value: 21.927472885137103\n",
      "State: 0, Optimal Action: 1, Value: 20.389742087251253\n",
      "State: 1, Optimal Action: 1, Value: 39.51424\n",
      "Iteration 8:\n",
      "State: -1, Optimal Action: 1, Value: 23.932913135688516\n",
      "State: 0, Optimal Action: 1, Value: 22.271715779223026\n",
      "State: 1, Optimal Action: 1, Value: 41.611392\n",
      "Iteration 9:\n",
      "State: -1, Optimal Action: 1, Value: 25.544576528351158\n",
      "State: 0, Optimal Action: 1, Value: 23.78445641514854\n",
      "State: 1, Optimal Action: 1, Value: 43.2891136\n",
      "Iteration 10:\n",
      "State: -1, Optimal Action: 1, Value: 26.836921596450033\n",
      "State: 0, Optimal Action: 1, Value: 24.997601769846423\n",
      "State: 1, Optimal Action: 1, Value: 44.63129088\n",
      "Iteration 11:\n",
      "State: -1, Optimal Action: 1, Value: 27.87204047838399\n",
      "State: 0, Optimal Action: 1, Value: 25.969335541706073\n",
      "State: 1, Optimal Action: 1, Value: 45.705032704000004\n",
      "Iteration 12:\n",
      "State: -1, Optimal Action: 1, Value: 28.7006480101707\n",
      "State: 0, Optimal Action: 1, Value: 26.747224541011526\n",
      "State: 1, Optimal Action: 1, Value: 46.564026163200005\n",
      "Iteration 13:\n",
      "State: -1, Optimal Action: 1, Value: 29.363745313148534\n",
      "State: 0, Optimal Action: 1, Value: 27.369742712170435\n",
      "State: 1, Optimal Action: 1, Value: 47.25122093056\n",
      "Iteration 14:\n",
      "State: -1, Optimal Action: 1, Value: 29.894310267109564\n",
      "State: 0, Optimal Action: 1, Value: 27.86784258541827\n",
      "State: 1, Optimal Action: 1, Value: 47.800976744448\n",
      "Iteration 15:\n",
      "State: -1, Optimal Action: 1, Value: 30.31879814716011\n",
      "State: 0, Optimal Action: 1, Value: 28.266357668955035\n",
      "State: 1, Optimal Action: 1, Value: 48.2407813955584\n",
      "Iteration 16:\n",
      "State: -1, Optimal Action: 1, Value: 30.65840326005418\n",
      "State: 0, Optimal Action: 1, Value: 28.585184242853067\n",
      "State: 1, Optimal Action: 1, Value: 48.592625116446726\n",
      "Iteration 17:\n",
      "State: -1, Optimal Action: 1, Value: 30.93009345619348\n",
      "State: 0, Optimal Action: 1, Value: 28.840251483367133\n",
      "State: 1, Optimal Action: 1, Value: 48.87410009315738\n",
      "Iteration 18:\n",
      "State: -1, Optimal Action: 1, Value: 31.147448130591357\n",
      "State: 0, Optimal Action: 1, Value: 29.044307741961905\n",
      "State: 1, Optimal Action: 1, Value: 49.09928007452591\n",
      "Iteration 19:\n",
      "State: -1, Optimal Action: 1, Value: 31.32133290809208\n",
      "State: 0, Optimal Action: 1, Value: 29.20755376566749\n",
      "State: 1, Optimal Action: 1, Value: 49.27942405962073\n",
      "Iteration 20:\n",
      "State: -1, Optimal Action: 1, Value: 31.4604411580622\n",
      "State: 0, Optimal Action: 1, Value: 29.33815100388007\n",
      "State: 1, Optimal Action: 1, Value: 49.42353924769659\n",
      "Iteration 21:\n",
      "State: -1, Optimal Action: 1, Value: 31.571727934494\n",
      "State: 0, Optimal Action: 1, Value: 29.442628967309926\n",
      "State: 1, Optimal Action: 1, Value: 49.53883139815727\n",
      "Iteration 22:\n",
      "State: -1, Optimal Action: 1, Value: 31.66075742839373\n",
      "State: 0, Optimal Action: 1, Value: 29.526211409325462\n",
      "State: 1, Optimal Action: 1, Value: 49.63106511852582\n",
      "Iteration 23:\n",
      "State: -1, Optimal Action: 1, Value: 31.73198105351077\n",
      "State: 0, Optimal Action: 1, Value: 29.59307739232384\n",
      "State: 1, Optimal Action: 1, Value: 49.70485209482066\n",
      "Iteration 24:\n",
      "State: -1, Optimal Action: 1, Value: 31.78895996597253\n",
      "State: 0, Optimal Action: 1, Value: 29.646570190838634\n",
      "State: 1, Optimal Action: 1, Value: 49.76388167585653\n",
      "Iteration 25:\n",
      "State: -1, Optimal Action: 1, Value: 31.83454310104144\n",
      "State: 0, Optimal Action: 1, Value: 29.689364434646045\n",
      "State: 1, Optimal Action: 1, Value: 49.81110534068523\n",
      "Iteration 26:\n",
      "State: -1, Optimal Action: 1, Value: 31.87100961119914\n",
      "State: 0, Optimal Action: 1, Value: 29.7235998317517\n",
      "State: 1, Optimal Action: 1, Value: 49.84888427254819\n",
      "Iteration 27:\n",
      "State: -1, Optimal Action: 1, Value: 31.900182820192203\n",
      "State: 0, Optimal Action: 1, Value: 29.75098815028546\n",
      "State: 1, Optimal Action: 1, Value: 49.87910741803856\n",
      "Iteration 28:\n",
      "State: -1, Optimal Action: 1, Value: 31.923521387744085\n",
      "State: 0, Optimal Action: 1, Value: 29.772898805462617\n",
      "State: 1, Optimal Action: 1, Value: 49.90328593443085\n",
      "Iteration 29:\n",
      "State: -1, Optimal Action: 1, Value: 31.942192241932965\n",
      "State: 0, Optimal Action: 1, Value: 29.79042732974871\n",
      "State: 1, Optimal Action: 1, Value: 49.92262874754468\n",
      "Iteration 30:\n",
      "State: -1, Optimal Action: 1, Value: 31.957128925344833\n",
      "State: 0, Optimal Action: 1, Value: 29.80445014923711\n",
      "State: 1, Optimal Action: 1, Value: 49.938102998035745\n",
      "Iteration 31:\n",
      "State: -1, Optimal Action: 1, Value: 31.969078272099377\n",
      "State: 0, Optimal Action: 1, Value: 29.815668404852374\n",
      "State: 1, Optimal Action: 1, Value: 49.9504823984286\n",
      "Iteration 32:\n",
      "State: -1, Optimal Action: 1, Value: 31.978637749513343\n",
      "State: 0, Optimal Action: 1, Value: 29.824643009354702\n",
      "State: 1, Optimal Action: 1, Value: 49.96038591874288\n",
      "Iteration 33:\n",
      "State: -1, Optimal Action: 1, Value: 31.98628533144878\n",
      "State: 0, Optimal Action: 1, Value: 29.83182269296074\n",
      "State: 1, Optimal Action: 1, Value: 49.96830873499431\n",
      "Iteration 34:\n",
      "State: -1, Optimal Action: 1, Value: 31.99240339699888\n",
      "State: 0, Optimal Action: 1, Value: 29.83756643984729\n",
      "State: 1, Optimal Action: 1, Value: 49.97464698799545\n",
      "Iteration 35:\n",
      "State: -1, Optimal Action: 1, Value: 31.997297849439686\n",
      "State: 0, Optimal Action: 1, Value: 29.842161437357237\n",
      "State: 1, Optimal Action: 1, Value: 49.97971759039636\n",
      "Iteration 36:\n",
      "State: -1, Optimal Action: 1, Value: 32.00121341139263\n",
      "State: 0, Optimal Action: 1, Value: 29.845837435365485\n",
      "State: 1, Optimal Action: 1, Value: 49.98377407231709\n",
      "Iteration 37:\n",
      "State: -1, Optimal Action: 1, Value: 32.004345860955105\n",
      "State: 0, Optimal Action: 1, Value: 29.848778233772208\n",
      "State: 1, Optimal Action: 1, Value: 49.987019257853675\n",
      "Iteration 38:\n",
      "State: -1, Optimal Action: 1, Value: 32.00685182060514\n",
      "State: 0, Optimal Action: 1, Value: 29.851130872497635\n",
      "State: 1, Optimal Action: 1, Value: 49.989615406282944\n",
      "Iteration 39:\n",
      "State: -1, Optimal Action: 1, Value: 32.00885658832519\n",
      "State: 0, Optimal Action: 1, Value: 29.853012983478003\n",
      "State: 1, Optimal Action: 1, Value: 49.99169232502636\n",
      "Iteration 40:\n",
      "State: -1, Optimal Action: 1, Value: 32.01046040250123\n",
      "State: 0, Optimal Action: 1, Value: 29.854518672262298\n",
      "State: 1, Optimal Action: 1, Value: 49.99335386002109\n",
      "Iteration 41:\n",
      "State: -1, Optimal Action: 1, Value: 32.01174345384207\n",
      "State: 0, Optimal Action: 1, Value: 29.855723223289736\n",
      "State: 1, Optimal Action: 1, Value: 49.99468308801687\n",
      "Iteration 42:\n",
      "State: -1, Optimal Action: 1, Value: 32.01276989491475\n",
      "State: 0, Optimal Action: 1, Value: 29.856686864111694\n",
      "State: 1, Optimal Action: 1, Value: 49.9957464704135\n",
      "Iteration 43:\n",
      "State: -1, Optimal Action: 1, Value: 32.01359104777289\n",
      "State: 0, Optimal Action: 1, Value: 29.85745777676926\n",
      "State: 1, Optimal Action: 1, Value: 49.9965971763308\n",
      "Iteration 44:\n",
      "State: -1, Optimal Action: 1, Value: 32.0142479700594\n",
      "State: 0, Optimal Action: 1, Value: 29.858074506895314\n",
      "State: 1, Optimal Action: 1, Value: 49.99727774106464\n",
      "Iteration 45:\n",
      "State: -1, Optimal Action: 1, Value: 32.014773507888606\n",
      "State: 0, Optimal Action: 1, Value: 29.85856789099615\n",
      "State: 1, Optimal Action: 1, Value: 49.997822192851714\n",
      "Iteration 46:\n",
      "State: -1, Optimal Action: 1, Value: 32.015193938151974\n",
      "State: 0, Optimal Action: 1, Value: 29.858962598276825\n",
      "State: 1, Optimal Action: 1, Value: 49.998257754281376\n",
      "Iteration 47:\n",
      "State: -1, Optimal Action: 1, Value: 32.015530282362676\n",
      "State: 0, Optimal Action: 1, Value: 29.859278364101364\n",
      "State: 1, Optimal Action: 1, Value: 49.998606203425105\n",
      "Iteration 48:\n",
      "State: -1, Optimal Action: 1, Value: 32.015799357731225\n",
      "State: 0, Optimal Action: 1, Value: 29.859530976760997\n",
      "State: 1, Optimal Action: 1, Value: 49.998884962740085\n",
      "Iteration 49:\n",
      "State: -1, Optimal Action: 1, Value: 32.01601461802608\n",
      "State: 0, Optimal Action: 1, Value: 29.8597330668887\n",
      "State: 1, Optimal Action: 1, Value: 49.99910797019207\n",
      "Iteration 50:\n",
      "State: -1, Optimal Action: 1, Value: 32.016186826261944\n",
      "State: 0, Optimal Action: 1, Value: 29.859894738990867\n",
      "State: 1, Optimal Action: 1, Value: 49.99928637615366\n",
      "Iteration 51:\n",
      "State: -1, Optimal Action: 1, Value: 32.01632459285065\n",
      "State: 0, Optimal Action: 1, Value: 29.860024076672595\n",
      "State: 1, Optimal Action: 1, Value: 49.99942910092293\n",
      "Iteration 52:\n",
      "State: -1, Optimal Action: 1, Value: 32.01643480612161\n",
      "State: 0, Optimal Action: 1, Value: 29.860127546817985\n",
      "State: 1, Optimal Action: 1, Value: 49.99954328073835\n",
      "Iteration 53:\n",
      "State: -1, Optimal Action: 1, Value: 32.01652297673838\n",
      "State: 0, Optimal Action: 1, Value: 29.860210322934293\n",
      "State: 1, Optimal Action: 1, Value: 49.99963462459068\n",
      "Iteration 54:\n",
      "State: -1, Optimal Action: 1, Value: 32.01659351323179\n",
      "State: 0, Optimal Action: 1, Value: 29.860276543827336\n",
      "State: 1, Optimal Action: 1, Value: 49.99970769967255\n",
      "Iteration 55:\n",
      "State: -1, Optimal Action: 1, Value: 32.016649942426525\n",
      "State: 0, Optimal Action: 1, Value: 29.860329520541775\n",
      "State: 1, Optimal Action: 1, Value: 49.999766159738044\n",
      "Iteration 56:\n",
      "State: -1, Optimal Action: 1, Value: 32.01669508578231\n",
      "State: 0, Optimal Action: 1, Value: 29.860371901913325\n",
      "State: 1, Optimal Action: 1, Value: 49.99981292779044\n",
      "Iteration 57:\n",
      "State: -1, Optimal Action: 1, Value: 32.016731200466936\n",
      "State: 0, Optimal Action: 1, Value: 29.86040580701056\n",
      "State: 1, Optimal Action: 1, Value: 49.99985034223235\n",
      "Iteration 58:\n",
      "State: -1, Optimal Action: 1, Value: 32.01676009221464\n",
      "State: 0, Optimal Action: 1, Value: 29.860432931088354\n",
      "State: 1, Optimal Action: 1, Value: 49.999880273785884\n",
      "Iteration 59:\n",
      "State: -1, Optimal Action: 1, Value: 32.016783205612796\n",
      "State: 0, Optimal Action: 1, Value: 29.860454630350585\n",
      "State: 1, Optimal Action: 1, Value: 49.99990421902871\n",
      "Iteration 60:\n",
      "State: -1, Optimal Action: 1, Value: 32.01680169633133\n",
      "State: 0, Optimal Action: 1, Value: 29.860471989760374\n",
      "State: 1, Optimal Action: 1, Value: 49.99992337522297\n",
      "Iteration 61:\n",
      "State: -1, Optimal Action: 1, Value: 32.01681648890615\n",
      "State: 0, Optimal Action: 1, Value: 29.860485877288202\n",
      "State: 1, Optimal Action: 1, Value: 49.999938700178376\n",
      "Iteration 62:\n",
      "State: -1, Optimal Action: 1, Value: 32.01682832296601\n",
      "State: 0, Optimal Action: 1, Value: 29.860496987310466\n",
      "State: 1, Optimal Action: 1, Value: 49.999950960142705\n",
      "Iteration 63:\n",
      "State: -1, Optimal Action: 1, Value: 32.0168377902139\n",
      "State: 0, Optimal Action: 1, Value: 29.860505875328275\n",
      "State: 1, Optimal Action: 1, Value: 49.999960768114164\n",
      "Iteration 64:\n",
      "State: -1, Optimal Action: 1, Value: 32.01684536401221\n",
      "State: 0, Optimal Action: 1, Value: 29.860512985742524\n",
      "State: 1, Optimal Action: 1, Value: 49.99996861449134\n",
      "Iteration 65:\n",
      "State: -1, Optimal Action: 1, Value: 32.01685142305085\n",
      "State: 0, Optimal Action: 1, Value: 29.860518674073923\n",
      "State: 1, Optimal Action: 1, Value: 49.99997489159307\n",
      "Iteration 66:\n",
      "State: -1, Optimal Action: 1, Value: 32.01685627028177\n",
      "State: 0, Optimal Action: 1, Value: 29.86052322473904\n",
      "State: 1, Optimal Action: 1, Value: 49.99997991327446\n",
      "Iteration 67:\n",
      "State: -1, Optimal Action: 1, Value: 32.01686014806651\n",
      "State: 0, Optimal Action: 1, Value: 29.86052686527114\n",
      "State: 1, Optimal Action: 1, Value: 49.99998393061957\n",
      "Iteration 68:\n",
      "State: -1, Optimal Action: 1, Value: 32.0168632502943\n",
      "State: 0, Optimal Action: 1, Value: 29.86052977769682\n",
      "State: 1, Optimal Action: 1, Value: 49.99998714449566\n",
      "Iteration 69:\n",
      "State: -1, Optimal Action: 1, Value: 32.01686573207653\n",
      "State: 0, Optimal Action: 1, Value: 29.86053210763736\n",
      "State: 1, Optimal Action: 1, Value: 49.999989715596534\n",
      "Iteration 70:\n",
      "State: -1, Optimal Action: 1, Value: 32.01686771750232\n",
      "State: 0, Optimal Action: 1, Value: 29.860533971589795\n",
      "State: 1, Optimal Action: 1, Value: 49.99999177247723\n",
      "Iteration 71:\n",
      "State: -1, Optimal Action: 1, Value: 32.016869305842945\n",
      "State: 0, Optimal Action: 1, Value: 29.860535462751734\n",
      "State: 1, Optimal Action: 1, Value: 49.999993417981784\n",
      "Iteration 72:\n",
      "State: -1, Optimal Action: 1, Value: 32.016870576515444\n",
      "State: 0, Optimal Action: 1, Value: 29.860536655681294\n",
      "State: 1, Optimal Action: 1, Value: 49.99999473438543\n",
      "Iteration 73:\n",
      "State: -1, Optimal Action: 1, Value: 32.016871593053445\n",
      "State: 0, Optimal Action: 1, Value: 29.860537610024938\n",
      "State: 1, Optimal Action: 1, Value: 49.99999578750835\n",
      "Iteration 74:\n",
      "State: -1, Optimal Action: 1, Value: 32.016872406283845\n",
      "State: 0, Optimal Action: 1, Value: 29.86053837349985\n",
      "State: 1, Optimal Action: 1, Value: 49.99999663000668\n",
      "Optimal Policy:\n",
      "State: -1, Optimal Action: 1\n",
      "State: 0, Optimal Action: 1\n",
      "State: 1, Optimal Action: 1\n",
      "{-1: 32.016872406283845, 0: 29.86053837349985, 1: 49.99999663000668}\n"
     ]
    }
   ],
   "source": [
    "# Define the states, actions, and next states\n",
    "states = [-1, 0, 1] #states = [\"Bear\", \"Flat\", \"Bull\"]\n",
    "\n",
    "actions = [-1, 0, 1] #actions = [\"Buy\", \"Sell\", \"Hold\"]\n",
    "\n",
    "next_states = [-1, 0, 1] #next_states=[\"Bear\",\"Bull\",\"Flat\"]\n",
    "\n",
    "\n",
    "\"\"\"rewards = {\n",
    "    \"Bear\": {\"Buy\": {\"Bear\": 10, \"Bull\": -5, \"Flat\": 0}, \"Sell\": {\"Bear\": -5, \"Bull\": 0, \"Flat\": 0}, \"Hold\": {\"Bear\": 0, \"Bull\": 0, \"Flat\": 0}},\n",
    "    \"Flat\": {\"Buy\": {\"Bear\": 0, \"Bull\": 0, \"Flat\": 0}, \"Sell\": {\"Bear\": 0, \"Bull\": 0, \"Flat\": 0}, \"Hold\": {\"Bear\": 0, \"Bull\": 0, \"Flat\": 0}},\n",
    "    \"Bull\": {\"Buy\": {\"Bear\": 0, \"Bull\": 0, \"Flat\": 0}, \"Sell\": {\"Bear\": 0, \"Bull\": 10, \"Flat\": 0}, \"Hold\": {\"Bear\": 0, \"Bull\": 0, \"Flat\": 0}}\n",
    "}\"\"\"\n",
    "# Define rewards with numeric values\n",
    "rewards = {\n",
    "    -1: {\n",
    "        -1: {-1: 10, 0: -5, 1: 0},\n",
    "        0: {-1: -5, 0: 0, 1: 0},\n",
    "        1: {-1: 0, 0: 0, 1: 0}\n",
    "    },\n",
    "    0: {\n",
    "        -1: { -1: 0, 0: 0, 1: 0},\n",
    "        0: { -1: 0, 0: 0, 1: 0},\n",
    "        1: { -1: 0, 0: 0, 1: 0}\n",
    "    },\n",
    "    1: {\n",
    "        -1: { -1: 0, 0: 0, 1: 0},\n",
    "        0: { -1: 0, 0: 0, 1: 0},\n",
    "        1: { -1: 0, 0: 0, 1: 10}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the transition probability dictionary\n",
    "transition_probabilities = {\n",
    "    state: {\n",
    "        action: {next_state: 0 for next_state in next_states}\n",
    "        for action in actions\n",
    "    }\n",
    "    for state in states\n",
    "}\n",
    "\n",
    "# Iterate over the transition counts to calculate probabilities\n",
    "for i, state in enumerate(states):\n",
    "    for j, action in enumerate(actions):\n",
    "        total_transitions = sum(transition_counts[(state, action)].values())\n",
    "        for k, next_state in enumerate(next_states):\n",
    "            transition_count = transition_counts[(state, action)].get(next_state, 0)\n",
    "            if total_transitions != 0:\n",
    "                transition_probabilities[state][action][next_state] = transition_count / total_transitions\n",
    "\n",
    "# Convert transition_probabilities to a numpy array for consistency\n",
    "transition_probabilities = np.array([[\n",
    "    [transition_probabilities[state][action][next_state] for next_state in next_states]\n",
    "    for action in actions]\n",
    "    for state in states])\n",
    "\n",
    "# Print the transition probability matrix\n",
    "print(\"Transition Probability Matrix:\")\n",
    "print(transition_probabilities)\n",
    "    \n",
    "\n",
    "# Initialize value function\n",
    "V = {state: 0 for state in states}\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.8\n",
    "\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Perform value iteration until convergence\n",
    "iteration = 0\n",
    "while True:\n",
    "    iteration += 1\n",
    "    delta = 0\n",
    "    print(f\"Iteration {iteration}:\")\n",
    "    for state in states:\n",
    "        old_value = V[state]\n",
    "        max_value = float('-inf')\n",
    "        max_action = None\n",
    "        #Using the Bellmans equation\n",
    "        for action in actions:\n",
    "            action_value = sum(transition_probabilities[state][action][next_state] *\n",
    "                               (rewards[state][action][next_state] + gamma * V[next_state])\n",
    "                               for next_state in states)\n",
    "            if action_value > max_value:\n",
    "                max_value = action_value\n",
    "                max_action = action\n",
    "        V[state] = max_value\n",
    "        delta = max(delta, abs(old_value - V[state]))\n",
    "        print(f\"State: {state}, Optimal Action: {max_action}, Value: {V[state]}\")\n",
    "    if delta < tolerance:\n",
    "        break\n",
    "\n",
    "# Derive optimal policy\n",
    "optimal_policy = {}\n",
    "for state in states:\n",
    "    max_action = None\n",
    "    max_action_value = float('-inf')\n",
    "    for action in actions:\n",
    "        action_value = sum(transition_probabilities[state][action][next_state] *\n",
    "                           (rewards[state][action][next_state] + gamma * V[next_state])\n",
    "                           for next_state in states)\n",
    "        if action_value > max_action_value:\n",
    "            max_action = action\n",
    "            max_action_value = action_value\n",
    "    optimal_policy[state] = max_action\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State: {state}, Optimal Action: {action}\")\n",
    "    \n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6a549c66-4947-41af-bb2b-d79196945702",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "State: Bear, Optimal Action: Buy, Value: 1.6666666666666665\n",
      "State: Bull, Optimal Action: Sell, Value: 3.7777777777777777\n",
      "State: Flat, Optimal Action: Buy, Value: 1.4518518518518517\n",
      "Iteration 2:\n",
      "State: Bear, Optimal Action: Buy, Value: 3.505679012345679\n",
      "State: Bull, Optimal Action: Sell, Value: 5.662748971193415\n",
      "State: Flat, Optimal Action: Buy, Value: 2.832074622770919\n",
      "Iteration 3:\n",
      "State: Bear, Optimal Action: Buy, Value: 4.866800695016003\n",
      "State: Bull, Optimal Action: Sell, Value: 6.896433143728089\n",
      "State: Flat, Optimal Action: Buy, Value: 3.8920822564040027\n",
      "Iteration 4:\n",
      "State: Bear, Optimal Action: Buy, Value: 5.841417625372825\n",
      "State: Bull, Optimal Action: Sell, Value: 7.767982140134644\n",
      "State: Flat, Optimal Action: Buy, Value: 4.667061872509725\n",
      "Iteration 5:\n",
      "State: Bear, Optimal Action: Buy, Value: 6.540389770137918\n",
      "State: Bull, Optimal Action: Sell, Value: 8.393449008741944\n",
      "State: Flat, Optimal Action: Buy, Value: 5.226906840370557\n",
      "Iteration 6:\n",
      "State: Bear, Optimal Action: Buy, Value: 7.042865498466779\n",
      "State: Bull, Optimal Action: Sell, Value: 8.843525692687809\n",
      "State: Flat, Optimal Action: Buy, Value: 5.630212808406705\n",
      "Iteration 7:\n",
      "State: Bear, Optimal Action: Buy, Value: 7.404427733216345\n",
      "State: Bull, Optimal Action: Sell, Value: 9.16751099581623\n",
      "State: Flat, Optimal Action: Buy, Value: 5.920573743317141\n",
      "Iteration 8:\n",
      "State: Bear, Optimal Action: Buy, Value: 7.664669992626591\n",
      "State: Bull, Optimal Action: Sell, Value: 9.40073459513599\n",
      "State: Flat, Optimal Action: Buy, Value: 6.129594221621259\n",
      "Iteration 9:\n",
      "State: Bear, Optimal Action: Buy, Value: 7.851999682502357\n",
      "State: Bull, Optimal Action: Sell, Value: 9.568620933135895\n",
      "State: Flat, Optimal Action: Buy, Value: 6.280057289935869\n",
      "Iteration 10:\n",
      "State: Bear, Optimal Action: Buy, Value: 7.986847441486433\n",
      "State: Bull, Optimal Action: Sell, Value: 9.689473510548853\n",
      "State: Flat, Optimal Action: Buy, Value: 6.3883675311923085\n",
      "Iteration 11:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.083916928860692\n",
      "State: Bull, Optimal Action: Sell, Value: 9.776468792160493\n",
      "State: Flat, Optimal Action: Buy, Value: 6.466334200590265\n",
      "Iteration 12:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.153791979096386\n",
      "State: Bull, Optimal Action: Sell, Value: 9.83909199249257\n",
      "State: Flat, Optimal Action: Buy, Value: 6.522458179247792\n",
      "Iteration 13:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.204091240223132\n",
      "State: Bull, Optimal Action: Sell, Value: 9.884171043190264\n",
      "State: Flat, Optimal Action: Buy, Value: 6.5628587900429824\n",
      "Iteration 14:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.2402989529217\n",
      "State: Bull, Optimal Action: Sell, Value: 9.916621009641318\n",
      "State: Flat, Optimal Action: Buy, Value: 6.591941000694933\n",
      "Iteration 15:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.266362923535453\n",
      "State: Bull, Optimal Action: Sell, Value: 9.939979982365788\n",
      "State: Flat, Optimal Action: Buy, Value: 6.612875708425646\n",
      "Iteration 16:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.285124963820504\n",
      "State: Bull, Optimal Action: Sell, Value: 9.95679484122985\n",
      "State: Flat, Optimal Action: Buy, Value: 6.627945470260267\n",
      "Iteration 17:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.298630740082833\n",
      "State: Bull, Optimal Action: Sell, Value: 9.968898947086121\n",
      "State: Flat, Optimal Action: Buy, Value: 6.638793375314458\n",
      "Iteration 18:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.308352816662243\n",
      "State: Bull, Optimal Action: Sell, Value: 9.97761203708342\n",
      "State: Flat, Optimal Action: Buy, Value: 6.646602194416033\n",
      "Iteration 19:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.315351212843119\n",
      "State: Bull, Optimal Action: Sell, Value: 9.983884118491352\n",
      "State: Flat, Optimal Action: Buy, Value: 6.652223340200134\n",
      "Iteration 20:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.320388979075895\n",
      "State: Bull, Optimal Action: Sell, Value: 9.988399050071301\n",
      "State: Flat, Optimal Action: Buy, Value: 6.656269698492622\n",
      "Iteration 21:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.324015394037286\n",
      "State: Bull, Optimal Action: Sell, Value: 9.991649104693655\n",
      "State: Flat, Optimal Action: Buy, Value: 6.65918245259295\n",
      "Iteration 22:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.326625853686371\n",
      "State: Bull, Optimal Action: Sell, Value: 9.993988642926128\n",
      "State: Flat, Optimal Action: Buy, Value: 6.661279186454786\n",
      "Iteration 23:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.328504982151276\n",
      "State: Bull, Optimal Action: Sell, Value: 9.995672749741917\n",
      "State: Flat, Optimal Action: Buy, Value: 6.662788511559461\n",
      "Iteration 24:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.329857664920707\n",
      "State: Bull, Optimal Action: Sell, Value: 9.996885046992556\n",
      "State: Flat, Optimal Action: Buy, Value: 6.66387499292606\n",
      "Iteration 25:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.330831387957153\n",
      "State: Bull, Optimal Action: Sell, Value: 9.997757714100205\n",
      "State: Flat, Optimal Action: Buy, Value: 6.664657091995577\n",
      "Iteration 26:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.331532318414116\n",
      "State: Bull, Optimal Action: Sell, Value: 9.998385899869307\n",
      "State: Flat, Optimal Action: Buy, Value: 6.665220082741067\n",
      "Iteration 27:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.332036880273197\n",
      "State: Bull, Optimal Action: Sell, Value: 9.998838096768953\n",
      "State: Flat, Optimal Action: Buy, Value: 6.665625349275524\n",
      "Iteration 28:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.332400087018048\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999163608816673\n",
      "State: Flat, Optimal Action: Buy, Value: 6.665917078696065\n",
      "Iteration 29:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.332661539874877\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999397927303363\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666127078899815\n",
      "Iteration 30:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.332849745620816\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999566600486398\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666278246668541\n",
      "Iteration 31:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.332985224740202\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999688019172037\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666387064154875\n",
      "Iteration 32:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333082748817898\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999775421905282\n",
      "State: Flat, Optimal Action: Buy, Value: 6.6664653959674816\n",
      "Iteration 33:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.33315295111751\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999838338397407\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666521782795306\n",
      "Iteration 34:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333203485949392\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999883628571228\n",
      "State: Flat, Optimal Action: Buy, Value: 6.66656237261758\n",
      "Iteration 35:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333239863236853\n",
      "State: Bull, Optimal Action: Sell, Value: 9.99991623051351\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666591591031452\n",
      "Iteration 36:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333266049275151\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999939698885363\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666612623784524\n",
      "Iteration 37:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333284899185344\n",
      "State: Bull, Optimal Action: Sell, Value: 9.99995659249473\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666627764123892\n",
      "Iteration 38:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333298468214391\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999968753288805\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666638662833891\n",
      "Iteration 39:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333308235823223\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999977507185578\n",
      "State: Flat, Optimal Action: Buy, Value: 6.6666465082247175\n",
      "Iteration 40:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333315266995605\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999983808641575\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666652155696506\n",
      "Iteration 41:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.33332032835565\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999988344718329\n",
      "State: Flat, Optimal Action: Buy, Value: 6.6666562210054625\n",
      "Iteration 42:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333323971754517\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999991609994215\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666659147401118\n",
      "Iteration 43:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.33332659443996\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999993960489412\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666661253954798\n",
      "Iteration 44:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333328482369112\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999995652483552\n",
      "State: Flat, Optimal Action: Buy, Value: 6.6666627703486565\n",
      "Iteration 45:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.33332984138702\n",
      "State: Bull, Optimal Action: Sell, Value: 9.99999687045846\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666663861918437\n",
      "Iteration 46:\n",
      "State: Bear, Optimal Action: Buy, Value: 8.333330819670378\n",
      "State: Bull, Optimal Action: Sell, Value: 9.999997747212605\n",
      "State: Flat, Optimal Action: Buy, Value: 6.666664647680378\n",
      "Optimal Policy:\n",
      "State: Bear, Optimal Action: Buy\n",
      "State: Bull, Optimal Action: Sell\n",
      "State: Flat, Optimal Action: Buy\n",
      "{'Bear': 8.333330819670378, 'Bull': 9.999997747212605, 'Flat': 6.666664647680378}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define states and actions\n",
    "states = ['Bear', 'Bull', 'Flat']\n",
    "actions = ['Buy', 'Sell', 'Hold']\n",
    "\n",
    "# Define rewards for each state-action pair\n",
    "rewards = {\n",
    "    'Bear': {\n",
    "        'Buy': {'Bear': 10, 'Bull': -5, 'Flat': 0},\n",
    "        'Sell': {'Bear': -5, 'Bull': 0, 'Flat': 0},\n",
    "        'Hold': {'Bear': 0, 'Bull': 0, 'Flat': 0}\n",
    "    },\n",
    "    'Bull': {\n",
    "        'Buy': {'Bear': 0, 'Bull': 0, 'Flat': 0},\n",
    "        'Sell': {'Bear': 0, 'Bull': 10, 'Flat': 0},\n",
    "        'Hold': {'Bear': 0, 'Bull': 0, 'Flat': 0}\n",
    "    },\n",
    "    'Flat': {\n",
    "        'Buy': {'Bear': 0, 'Bull': 0, 'Flat': 0},\n",
    "        'Sell': {'Bear': 0, 'Bull': 0, 'Flat': 0},\n",
    "        'Hold': {'Bear': 0, 'Bull': 0, 'Flat': 0}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define transition probabilities (assuming uniform distribution)\n",
    "transition_probabilities = {\n",
    "    state: {\n",
    "        action: {next_state: 1/len(states) for next_state in states}\n",
    "        for action in actions\n",
    "    }\n",
    "    for state in states\n",
    "}\n",
    "\n",
    "# Initialize value function\n",
    "V = {state: 0 for state in states}\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.8\n",
    "\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Perform value iteration until convergence\n",
    "iteration = 0\n",
    "while True:\n",
    "    iteration += 1\n",
    "    delta = 0\n",
    "    print(f\"Iteration {iteration}:\")\n",
    "    for state in states:\n",
    "        old_value = V[state]\n",
    "        max_value = float('-inf')\n",
    "        max_action = None\n",
    "        # Using the Bellman equation\n",
    "        for action in actions:\n",
    "            action_value = sum(transition_probabilities[state][action][next_state] *\n",
    "                               (rewards[state][action][next_state] + gamma * V[next_state])\n",
    "                               for next_state in states)\n",
    "            if action_value > max_value:\n",
    "                max_value = action_value\n",
    "                max_action = action\n",
    "        V[state] = max_value\n",
    "        delta = max(delta, abs(old_value - V[state]))\n",
    "        print(f\"State: {state}, Optimal Action: {max_action}, Value: {V[state]}\")\n",
    "    if delta < tolerance:\n",
    "        break\n",
    "\n",
    "# Derive optimal policy\n",
    "optimal_policy = {}\n",
    "for state in states:\n",
    "    max_action = None\n",
    "    max_action_value = float('-inf')\n",
    "    for action in actions:\n",
    "        action_value = sum(transition_probabilities[state][action][next_state] *\n",
    "                           (rewards[state][action][next_state] + gamma * V[next_state])\n",
    "                           for next_state in states)\n",
    "        if action_value > max_action_value:\n",
    "            max_action = action\n",
    "            max_action_value = action_value\n",
    "    optimal_policy[state] = max_action\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State: {state}, Optimal Action: {action}\")\n",
    "    \n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4128d507-0056-456c-afba-fefaa132ab0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Actions:\n",
      "State: Bear, Optimal Action: Buy\n",
      "State: Bull, Optimal Action: Sell\n",
      "State: Flat, Optimal Action: Buy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define states and actions\n",
    "states = ['Bear', 'Bull', 'Flat']\n",
    "actions = ['Buy', 'Sell', 'Hold']\n",
    "\n",
    "# Define rewards for each state-action pair\n",
    "rewards = {\n",
    "    'Bear': {\n",
    "        'Buy': {'Bear': -5, 'Bull': +10, 'Flat': +5},\n",
    "        'Sell': {'Bear': +10, 'Bull': -5, 'Flat': +5},\n",
    "        'Hold': {'Bear': -5, 'Bull': +10, 'Flat': +5}\n",
    "    },\n",
    "    'Bull': {\n",
    "        'Buy': {'Bear': 0, 'Bull': 0, 'Flat': 0},\n",
    "        'Sell': {'Bear': 0, 'Bull': 10, 'Flat': 0},\n",
    "        'Hold': {'Bear': 0, 'Bull': 0, 'Flat': 0}\n",
    "    },\n",
    "    'Flat': {\n",
    "        'Buy': {'Bear': 0, 'Bull': 0, 'Flat': 0},\n",
    "        'Sell': {'Bear': 0, 'Bull': 0, 'Flat': 0},\n",
    "        'Hold': {'Bear': 0, 'Bull': 0, 'Flat': 0}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define transition probabilities (assuming uniform distribution)\n",
    "transition_probabilities = {\n",
    "    'Bear': {\n",
    "        'Buy': {'Bear': 0.1, 'Bull': 0.7, 'Flat': 0.2},\n",
    "        'Sell': {'Bear': 0.6, 'Bull': 0.3, 'Flat': 0.1},\n",
    "        'Hold': {'Bear': 0.4, 'Bull': 0.4, 'Flat': 0.2}\n",
    "    },\n",
    "    'Bull': {\n",
    "        'Buy': {'Bear': 0.3, 'Bull': 0.4, 'Flat': 0.3},\n",
    "        'Sell': {'Bear': 0.2, 'Bull': 0.5, 'Flat': 0.3},\n",
    "        'Hold': {'Bear': 0.1, 'Bull': 0.6, 'Flat': 0.3}\n",
    "    },\n",
    "    'Flat': {\n",
    "        'Buy': {'Bear': 0.2, 'Bull': 0.6, 'Flat': 0.2},\n",
    "        'Sell': {'Bear': 0.3, 'Bull': 0.4, 'Flat': 0.3},\n",
    "        'Hold': {'Bear': 0.3, 'Bull': 0.3, 'Flat': 0.4}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.8\n",
    "\n",
    "# Convergence threshold\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Initialize value function and optimal actions\n",
    "V = {state: 0 for state in states}\n",
    "optimal_actions = {state: None for state in states}\n",
    "\n",
    "# Perform value iteration until convergence\n",
    "while True:\n",
    "    delta = 0\n",
    "    for state in states:\n",
    "        old_value = V[state]\n",
    "        max_value = float('-inf')\n",
    "        max_action = None\n",
    "        # Calculate the value for each action and choose the one with the maximum value\n",
    "        for action in actions:\n",
    "            action_value = sum(transition_probabilities[state][action][next_state] *\n",
    "                               (rewards[state][action][next_state] + gamma * V[next_state])\n",
    "                               for next_state in states)\n",
    "            if action_value > max_value:\n",
    "                max_value = action_value\n",
    "                max_action = action\n",
    "        V[state] = max_value\n",
    "        optimal_actions[state] = max_action\n",
    "        delta = max(delta, abs(old_value - V[state]))\n",
    "    if delta < tolerance:\n",
    "        break\n",
    "\n",
    "# Print the optimal actions for each state\n",
    "print(\"Optimal Actions:\")\n",
    "for state, action in optimal_actions.items():\n",
    "    print(f\"State: {state}, Optimal Action: {action}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
